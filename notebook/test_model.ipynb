{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import function,grad,pp,shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "ds = datasets.load_iris()\n",
    "train_data_x = ds['data'][:,0:2]\n",
    "train_data_y = ds['target']\n",
    "\n",
    "train_data_x = np.append(train_data_x[train_data_y == 0],train_data_x[train_data_y == 1],axis=0)\n",
    "train_data_y = np.array(np.append(train_data_y[train_data_y == 0],train_data_y[train_data_y == 1]),\n",
    "                        dtype=theano.config.floatX)\n",
    "train_set_x = shared(value=train_data_x,name='train_set_x',borrow=True)\n",
    "train_set_y = shared(value=train_data_y,name='train_set_y',borrow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self,n_in,n_out):\n",
    "        self.x = T.matrix('x')\n",
    "        self.y = T.dvector('y')\n",
    "        self.W = shared(\n",
    "            value=np.random.uniform(low=-0.1,high=0.1,size=(n_in,n_out)),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = shared(\n",
    "            value=np.random.uniform(low=-0.1,high=0.1,size=(n_out,)),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.cost = T.mean( (self.y.reshape((batch_size,1)) - T.nnet.sigmoid(T.dot(self.x ,self.W) + self.b)) ** 2)\n",
    "        self.output = None\n",
    "    \n",
    "learning_rate = 0.1\n",
    "index = T.iscalar('index')\n",
    "model = LogisticRegression(2,1)\n",
    "g_w = grad(model.cost,model.W)\n",
    "g_b = grad(model.cost,model.b)\n",
    "\n",
    "train = function(inputs=[index],outputs=model.cost,\n",
    "         updates=[\n",
    "                 (model.W, model.W - learning_rate * g_w),\n",
    "                 (model.b, model.b - learning_rate * g_b)\n",
    "                 ],\n",
    "         givens=[(model.x,train_set_x[index * batch_size :(index+1) * batch_size ]),\n",
    "                 (model.y,train_set_y[index * batch_size :(index+1) * batch_size ])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prev_cost = train(0)\n",
    "for i in range(100000):\n",
    "    cost = train(0)\n",
    "    if np.abs(prev_cost - cost) < 0.001:\n",
    "        break\n",
    "    \n",
    "print train(0)\n",
    "\n",
    "plt.plot(train_data_x[train_data_y==0][:,0],train_data_x[train_data_y==0][:,1],'+r')\n",
    "plt.plot(train_data_x[train_data_y==1][:,0],train_data_x[train_data_y==1][:,1],'+b')\n",
    "params = model.W.get_value()\n",
    "bias = model.b.get_value()\n",
    "x = np.arange(np.min(train_data_x[:,0]),np.max(train_data_x[:,0]),0.05)\n",
    "y = (params[0]*x+bias)/-params[1]\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
