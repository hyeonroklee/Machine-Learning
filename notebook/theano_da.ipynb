{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import gzip,cPickle,sys,os\n",
    "from PIL import Image\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano import shared,function,grad,pp\n",
    "from theano.tensor.shared_randomstreams import RandomStreams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = gzip.open('mnist.pkl.gz','rb')\n",
    "train_set,valid_set,test_set = cPickle.load(f)\n",
    "f.close()\n",
    "def share_dataset(data_xy):\n",
    "    data_x,data_y = data_xy\n",
    "    shared_x = shared(np.array(data_x,dtype=theano.config.floatX))\n",
    "    shared_y = shared(np.array(data_y,dtype=theano.config.floatX))\n",
    "    return shared_x,T.cast(shared_y,'int32')\n",
    "\n",
    "train_set_x,train_set_y = share_dataset(train_set)\n",
    "valid_set_x,valid_set_y = share_dataset(valid_set)\n",
    "test_set_x,test_set_y = share_dataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self,x,n_in,n_out):\n",
    "        self.W = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_in,n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(x,self.W)+self.b)\n",
    "        self.y_pred = T.argmax(self.p_y_given_x,axis=1)\n",
    "        self.params = [ self.W, self.b ]\n",
    "        self.x = x\n",
    "    \n",
    "    def negative_log_likelihood(self,y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]),y])\n",
    "    \n",
    "    def errors(self,y):\n",
    "        return T.mean(T.neq(self.y_pred,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self,x,n_in,n_out):\n",
    "        self.x = x\n",
    "        W_values = np.asarray(\n",
    "                np.random.uniform(\n",
    "                    low=-np.sqrt(6./(n_in + n_out)),\n",
    "                    high=np.sqrt(6./(n_in + n_out)),\n",
    "                    size=(n_in,n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "        )\n",
    "        W = shared(value=W_values, name='W',borrow=True)\n",
    "        b_values = np.zeros((n_out,),dtype=theano.config.floatX)\n",
    "        b = shared(value=b_values,name='b',borrow=True)\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.output = T.tanh(T.dot(x,self.W) + self.b)\n",
    "        self.params = [ self.W, self.b ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DA(object):\n",
    "    def __init__(self,input,n_visible,n_hidden,W=None,bhid=None,bvis=None):\n",
    "        self.x = input\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        if not W:\n",
    "            _W = np.asarray(\n",
    "                np.random.uniform(\n",
    "                    low=-4*np.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4*np.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible,n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            self.W = shared(value=_W,name='W',borrow=True)\n",
    "        else:\n",
    "            self.W = W\n",
    "        \n",
    "        if not bhid:\n",
    "            self.b = shared(\n",
    "                value=np.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='b',\n",
    "                borrow=True\n",
    "            )\n",
    "        else:\n",
    "            self.b = bhid\n",
    "            \n",
    "        self.W_prime = self.W.T\n",
    "        \n",
    "        if not bvis:\n",
    "            self.b_prime = shared(\n",
    "                value=np.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            self.b_prime = bvis\n",
    "        \n",
    "        self.params = [self.W, self.b, self.b_prime]\n",
    "        self.theano_rng = RandomStreams(np.random.randint(2 ** 30))\n",
    "\n",
    "    def get_hidden_values(self,input):\n",
    "        return T.nnet.sigmoid(T.dot(input,self.W) + self.b)\n",
    "    \n",
    "    def get_reconstructed_input(self,hidden):\n",
    "        return T.nnet.sigmoid(T.dot(hidden,self.W_prime) + self.b_prime)\n",
    "    \n",
    "    def get_cost_updates(self,corruption_level,learning_rate):\n",
    "        corrupted_x = self.get_corrupted_input(self.x, corruption_level)\n",
    "        y = self.get_hidden_values(corrupted_x)\n",
    "        z = self.get_reconstructed_input(y)\n",
    "        L = -  T.sum(self.x * T.log(z) + (1-self.x) * T.log(1-z),axis=1)\n",
    "        cost = T.mean(L)\n",
    "        gparams = grad(cost,self.params)\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(self.params,gparams)\n",
    "        ]\n",
    "        return (cost,updates)\n",
    "    \n",
    "    def get_corrupted_input(self, input, corruption_level):\n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training epoch 0, cost ', 126.56361769299292)\n",
      "('Training epoch 1, cost ', 94.774712825267017)\n",
      "('Training epoch 2, cost ', 88.526435498703464)\n",
      "('Training epoch 3, cost ', 85.205810665698252)\n",
      "('Training epoch 4, cost ', 83.041297199536899)\n",
      "('Training epoch 5, cost ', 81.568192024923448)\n",
      "('Training epoch 6, cost ', 80.44665463393379)\n",
      "('Training epoch 7, cost ', 79.59331354841602)\n",
      "('Training epoch 8, cost ', 78.882518895178038)\n",
      "('Training epoch 9, cost ', 78.294319900514495)\n",
      "training time is 1.56m\n"
     ]
    }
   ],
   "source": [
    "batch_size = 600\n",
    "index = T.lscalar()\n",
    "x = T.matrix('x')\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "da = DA(input=x,n_visible=28*28,n_hidden=500)\n",
    "cost,updates = da.get_cost_updates(\n",
    "    corruption_level=0.3,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "train_da = function(\n",
    "    [index],\n",
    "    cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_set_x[index * batch_size: (index+1)* batch_size]\n",
    "    }\n",
    ")\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "for epoch in range(10):\n",
    "    c = []\n",
    "    for batch_index in range(n_train_batches):\n",
    "        c.append(train_da(batch_index))\n",
    "    print('Training epoch %d, cost ' % epoch, np.mean(c))\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "training_time = (end_time - start_time)\n",
    "\n",
    "print(('training time is %.2fm' % (training_time / 60.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SDA(object):\n",
    "    def __init__(self,\n",
    "                 n_in=784,\n",
    "                 hidden_layers_sizes=[500,500],\n",
    "                 n_outs=10,\n",
    "                 corruption_levels=[0.1,0.1] ):\n",
    "        self.sigmoid_layers = []\n",
    "        self.da_layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "        \n",
    "        self.theano_rng = RandomStreams(np.random.randint(2**30))\n",
    "        self.x = T.matrix('x')\n",
    "        self.y = T.ivector('y')\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            if i == 0:\n",
    "                input_size = n_in\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i-1]\n",
    "                layer_input = self.sigmoid_layers[-1].output\n",
    "        \n",
    "            sigmoid_layer = HiddenLayer(x=layer_input,\n",
    "                                        n_in=input_size,\n",
    "                                        n_out=hidden_layers_sizes[i])\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "            self.params.extend(sigmoid_layer.params)\n",
    "            \n",
    "            dA_layer = DA(input=layer_input,\n",
    "                          n_visible=input_size,\n",
    "                          n_hidden=hidden_layers_sizes[i],\n",
    "                          W=sigmoid_layer.W,\n",
    "                          bhid=sigmoid_layer.b)\n",
    "            self.da_layers.append(dA_layer)\n",
    "\n",
    "        self.logLayer = LogisticRegression(\n",
    "            x=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1],\n",
    "            n_out=n_outs\n",
    "        )\n",
    "        self.params.extend(self.logLayer.params)\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
    "        self.errors = self.logLayer.errors(self.y)\n",
    "    \n",
    "    def pretraining_functions(self, train_set_x, batch_size):\n",
    "        index = T.lscalar('index')\n",
    "        corruption_level = T.scalar('corruption')  # % of corruption to use\n",
    "        learning_rate = T.scalar('lr')  # learning rate to use\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "\n",
    "        pretrain_fns = []\n",
    "        for dA in self.da_layers:\n",
    "            # get the cost and the updates list\n",
    "            cost, updates = dA.get_cost_updates(corruption_level,\n",
    "                                                learning_rate)\n",
    "            # compile the theano function\n",
    "            fn = theano.function(\n",
    "                inputs=[\n",
    "                    index,\n",
    "                    theano.In(corruption_level, value=0.2),\n",
    "                    theano.In(learning_rate, value=0.1)\n",
    "                ],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    self.x: train_set_x[batch_begin: batch_end]\n",
    "                }\n",
    "            )\n",
    "            # append `fn` to the list of functions\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "    \n",
    "    def build_finetune_functions(self, batch_size, learning_rate):\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_valid_batches //= batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_batches //= batch_size\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # compute list of fine-tuning updates\n",
    "        updates = [\n",
    "            (param, param - gparam * learning_rate)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: train_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='train'\n",
    "        )\n",
    "\n",
    "        test_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='test'\n",
    "        )\n",
    "\n",
    "        valid_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: valid_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: valid_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='valid'\n",
    "        )\n",
    "\n",
    "        # Create a function that scans the entire validation set\n",
    "        def valid_score():\n",
    "            return [valid_score_i(i) for i in range(n_valid_batches)]\n",
    "\n",
    "        # Create a function that scans the entire test set\n",
    "        def test_score():\n",
    "            return [test_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        return train_fn, valid_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... getting the pretraining functions\n",
      "... pre-training the model\n",
      "Pre-training layer 0, epoch 0, cost 144.305060\n",
      "Pre-training layer 0, epoch 1, cost 101.219341\n",
      "Pre-training layer 1, epoch 0, cost -184724.699653\n",
      "Pre-training layer 1, epoch 1, cost -559510.676238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<theano.compile.function_module.Function at 0x10c1b2c50>,\n",
       " <function __main__.valid_score>,\n",
       " <function __main__.test_score>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sda = SDA()\n",
    "\n",
    "print('... getting the pretraining functions')\n",
    "pretraining_fns = sda.pretraining_functions(train_set_x=train_set_x,\n",
    "                                                batch_size=batch_size)\n",
    "\n",
    "print('... pre-training the model')\n",
    "start_time = timeit.default_timer()\n",
    "corruption_levels = [.1, .2, .3]\n",
    "for i in range(sda.n_layers):\n",
    "    for epoch in range(2):\n",
    "        c = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            c.append(pretraining_fns[i](index=batch_index,\n",
    "                         corruption=corruption_levels[i],\n",
    "                         lr=0.1))\n",
    "        print('Pre-training layer %i, epoch %d, cost %f' % (i, epoch, np.mean(c)))\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fn, valid_score, test_score = sda.build_finetune_functions(batch_size=600,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9071875\n",
      "8.34176133513\n",
      "8.9838537059\n",
      "9.68613665453\n",
      "9.93588579311\n",
      "10.0647988361\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-097975ae1e23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/theano/tensor/blas.pyc\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we mean to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print np.mean(test_score())\n",
    "\n",
    "for epoch in range(10):\n",
    "    costs = [] \n",
    "    for i in range(n_train_batches):\n",
    "        costs.append(train_fn(i))\n",
    "    print np.mean(costs)\n",
    "        \n",
    "print np.mean(test_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
