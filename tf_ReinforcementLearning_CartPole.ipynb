{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as opt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-07-27 22:33:29,893] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "lr = 0.05\n",
    "hidden = 100\n",
    "num_action = env.action_space.n\n",
    "num_state = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,num_state])\n",
    "w1 = tf.Variable(np.random.randn(num_state,hidden),name='w1',dtype=tf.float32)\n",
    "b1 = tf.Variable(np.random.randn(1,hidden),name='b1',dtype=tf.float32)\n",
    "layer1 = tf.nn.relu(tf.matmul(x,w1) + b1)\n",
    "w2 = tf.Variable(np.random.randn(hidden,num_action),name='w2',dtype=tf.float32)\n",
    "b2 = tf.Variable(np.random.randn(1,num_action),name='b2',dtype=tf.float32)\n",
    "layer2 = tf.nn.sigmoid(tf.matmul(layer1,w2) + b2)\n",
    "output = tf.nn.softmax(layer2)\n",
    "\n",
    "chosen_action = tf.argmax(output,axis=1)\n",
    "\n",
    "action_holder = tf.placeholder(tf.float32,[None,num_action])\n",
    "reward_holder = tf.placeholder(tf.float32,[None])\n",
    "\n",
    "cost = -tf.reduce_mean(tf.log(tf.reduce_sum(output * action_holder,axis=1)) * \n",
    "                      reward_holder)\n",
    "\n",
    "gradient_holders = []\n",
    "gradient_buffer = []\n",
    "tvars = tf.trainable_variables()\n",
    "for idx,var in enumerate(tvars):\n",
    "    ph = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "    gradient_holders.append(ph)\n",
    "    gradient_buffer.append(0)\n",
    "\n",
    "grad = tf.gradients(cost,tvars)\n",
    "opt = tf.train.AdamOptimizer(0.05)\n",
    "update = opt.apply_gradients(zip(gradient_holders,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "11.0\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "e = 0.1\n",
    "epoch = 10000\n",
    "init = tf.global_variables_initializer()\n",
    "amap = np.identity(num_action)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(epoch):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        s_hist = []\n",
    "        a_hist = []\n",
    "        r_hist = []\n",
    "        while not done:\n",
    "            a = sess.run(chosen_action,{x:np.array([s])})\n",
    "            if np.random.rand() < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "            s1,r,done,_ = env.step(a[0])\n",
    "            s_hist.append(s)\n",
    "            a_hist.append(amap[a[0]:a[0]+1][0])\n",
    "            r_hist.append(r)\n",
    "            total_reward += r\n",
    "            if done:\n",
    "                s_h = np.array(s_hist,dtype=np.float32)\n",
    "                a_h = np.array(a_hist,dtype=np.float32)\n",
    "                r_h = discount_rewards(np.array(r_hist,dtype=np.float32))\n",
    "                c,g = sess.run([cost,grad],{x:s_h,action_holder:a_h,\n",
    "                                       reward_holder:r_h})\n",
    "\n",
    "                for ix,gd in enumerate(g):\n",
    "                    gradient_buffer[ix] += gd\n",
    "                    \n",
    "                if i % 1000 == 0:\n",
    "                    sess.run(update,feed_dict=dict(zip(gradient_holders, gradient_buffer)))\n",
    "                    for ix,gd in enumerate(g):\n",
    "                        gradient_buffer[ix] = 0\n",
    "                    print total_reward\n",
    "            s = s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
